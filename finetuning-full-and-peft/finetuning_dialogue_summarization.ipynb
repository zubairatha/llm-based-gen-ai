{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvCZUt8hE8HQ",
        "tags": []
      },
      "source": [
        "# Fine-Tune a Generative AI Model for Dialogue Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEdmJvnhE8HR"
      },
      "source": [
        "In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWgiJEWaE8HR"
      },
      "source": [
        "# Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXRNJdPsE8HS",
        "tags": []
      },
      "source": [
        "- [ 1 - Load Required Dependencies, Dataset and LLM](#1)\n",
        "  - [ 1.1 - Set up Required Dependencies](#1.1)\n",
        "  - [ 1.2 - Load Dataset and LLM](#1.2)\n",
        "  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n",
        "- [ 2 - Perform Full Fine-Tuning](#2)\n",
        "  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n",
        "  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n",
        "  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n",
        "  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n",
        "- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n",
        "  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n",
        "  - [ 3.2 - Train PEFT Adapter](#3.2)\n",
        "  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n",
        "  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfmOkoOWE8HS"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Load Required Dependencies, Dataset and LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjpKw59QE8HS"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "### 1.1 - Set up Required Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0OoSUyfE8HT",
        "tags": []
      },
      "source": [
        "Now install the required packages for the LLM and datasets.\n",
        "\n",
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVxflYZ0E8HT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip -q install datasets transformers evaluate rouge_score peft loralib accelerate sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpDYl-JAE8HT",
        "tags": []
      },
      "source": [
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm116kJCE8HT",
        "tags": []
      },
      "source": [
        "Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2pqFYfvE8HT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "import evaluate  # for ROUGE and other metrics\n",
        "\n",
        "# PEFT / LoRA\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txELz8o8E8HT",
        "tags": []
      },
      "source": [
        "<a name='1.2'></a>\n",
        "### 1.2 - Load Dataset and LLM\n",
        "\n",
        "You are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpxb5R0mE8HT",
        "outputId": "dc7bcf79-9c7b-4143-dcc6-e4771e9a1030",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToPcIKekE8HT",
        "tags": []
      },
      "source": [
        "Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-small) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrhQB3peE8HT",
        "outputId": "06831e27-80c1-42b3-cff0-67844e952152",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "model_name = \"google/flan-t5-small\"\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "train_model = copy.deepcopy(original_model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6t-ikBAE8HT",
        "tags": []
      },
      "source": [
        "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI8dZe_zE8HT",
        "outputId": "34c0dc23-9429-4983-c16d-b92d6f26010f",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters:     76,961,152\n",
            "Trainable parameters: 76,961,152 (100.00%)\n",
            "{'total_params': 76961152, 'trainable_params': 76961152, 'trainable_pct': 100.0}\n"
          ]
        }
      ],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for p in model.parameters():\n",
        "        num = p.numel()\n",
        "        all_model_params += num\n",
        "        if p.requires_grad:\n",
        "            trainable_model_params += num\n",
        "    pct = (trainable_model_params / all_model_params) * 100 if all_model_params else 0.0\n",
        "    print(f\"Total parameters:     {all_model_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_model_params:,} ({pct:.2f}%)\")\n",
        "    return {\n",
        "        \"total_params\": all_model_params,\n",
        "        \"trainable_params\": trainable_model_params,\n",
        "        \"trainable_pct\": pct,\n",
        "    }\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(original_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpt_crIEE8HT",
        "tags": []
      },
      "source": [
        "<a name='1.3'></a>\n",
        "### 1.3 - Test the Model with Zero Shot Inferencing\n",
        "\n",
        "Test the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ETFeRi-E8HT",
        "outputId": "905d4a20-470e-4c50-b2fa-d8cc47059024",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "Summarize the following dialogue:\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "Summary:\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "How would you like to add a CD-ROM drive?\n"
          ]
        }
      ],
      "source": [
        "index = 200\n",
        "\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"Summarize the following dialogue:\\n{dialogue}\\nSummary:\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(original_model.device)\n",
        "\n",
        "output = tokenizer.decode(\n",
        "    original_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAstf87COqdo"
      },
      "source": [
        "* The model captures a small detail from the conversation (mention of the CD-ROM drive) but fails to provide a full summary of the dialogue’s main idea.\n",
        "* Compared to the human summary, it lacks abstraction and misses key points about system upgrades and the instructional nature of the conversation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8gD7wGQE8HU",
        "tags": []
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Perform Full Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9CchIVtE8HU",
        "tags": []
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 - Preprocess the Dialog-Summary Dataset\n",
        "\n",
        "You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary` as follows:\n",
        "\n",
        "Training prompt (dialogue):\n",
        "```\n",
        "Summarize the following conversation.\n",
        "\n",
        "    Chris: This is his part of the conversation.\n",
        "    Antje: This is her part of the conversation.\n",
        "    \n",
        "Summary:\n",
        "```\n",
        "\n",
        "Training response (summary):\n",
        "```\n",
        "Both Chris and Antje participated in the conversation.\n",
        "```\n",
        "\n",
        "Then preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f21a7234c9ca43edacf1b03273397104",
            "509e243000c14d6ca05af965c7e14d5c",
            "2fbdb2c5cb8149068e097ada3221e60d",
            "76fdb2ec372a49908a14376d7ef64d5f",
            "a3cda832b276441ca531891e79a40877",
            "dcb64abd44144b848992ba67e75aff2a",
            "9d8206dce5f64c7290e9a39a81554ee7",
            "2fcd598963784592aff8de79099c5383",
            "7a35ead1a9bc4e99a9efd5a3d2aa8a00",
            "f1a3a73fbb3541d2b633c7b5760d9f4c",
            "02417dda2c2a463eb3886a0a3596836a"
          ]
        },
        "id": "ELkfDGzdE8HU",
        "outputId": "dce39642-4ed6-476d-b07e-255e78a90fe4",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f21a7234c9ca43edacf1b03273397104",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize_function(example):\n",
        "    # Build instruction-style prompt and target\n",
        "    prompt = (\n",
        "        \"Summarize the following conversation.\\n\\n\"\n",
        "        f\"{example['dialogue']}\\n\\n\"\n",
        "        \"Summary:\"\n",
        "    )\n",
        "    target = example[\"summary\"]\n",
        "\n",
        "    # Tokenize source (dialogue prompt) and target (summary)\n",
        "    # Use truncation; padding will be handled later by a data collator\n",
        "    model_inputs = tokenizer(\n",
        "        prompt,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        text_target=target,\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=False,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGBw24dFE8HU",
        "tags": []
      },
      "source": [
        "To save some time in the lab, you will subsample the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "3d462b147ca24e86baec72410a98a48a",
            "af3eb36c95844729aa02e1d45e744b07",
            "b411b01ec5544a03a37480ff486e6649",
            "40401be4ab774ca581cef4ecea6d09f5",
            "a10769ea271c4d2f9ee76c50e3b417e4",
            "8a84bc0015e74b3c8cd337897f28b572",
            "c9fe8cf16a55403b94b9b06eff4ada28",
            "415762945a0c48308adaa29b6ce7f74f",
            "4f733ba5662f44d4bc47ddde33f71a56",
            "3ebbac34dad2492d91df8e9fa3790a9f",
            "aa406bb7d59b42eda7a9049830257503",
            "c080feb1d6f04bd081d99ebc3d809eb1",
            "70e6bf694a78425ebca526376d090744",
            "940e8a0b4232469a9ac4bd206bb7dec0",
            "29fb69504ff9405b8fd81e5ca6f57d70",
            "3bce58e8e3944dbd9c275fb1bb9d2bab",
            "9410398ecda2409584fe92a00e02c426",
            "2ce72f50038e4d0d84c0c022e05e90eb",
            "fbf269c44f7f422e9df9132b01bc99d3",
            "64f9759fd57c4afa95b984edebf319f7",
            "fda50113aaf449ffa5cc67aea81e06b3",
            "1c5fc23f1534459aa8c9b3576a723689",
            "6405f67089c0463fa2f320f4f9359ae4",
            "aa9d161841a54d568ec1757e8596cf08",
            "5ec502a62e8a45faba52ea99c9c10bfa",
            "0b895f533d3d4dfd87cf9ff7759a4c6e",
            "19eb1b4f77a94691b88cef2a8de8f097",
            "b01d3b7a276344528de145e07e73a98f",
            "4474188b650e4b048e934fb6bc66454d",
            "5e4ec3ab9c004ee4bff8004109e5ce0a",
            "8d015f1f922a4441905d769dac14fb4b",
            "e73fd41e0c854c7abaedf8c8c04528d5",
            "f180ea1775e54f1e8bf809878413a3a9"
          ]
        },
        "id": "wWkH9OBXE8HU",
        "outputId": "c3950836-9df8-44e8-841f-9d187933fcde",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d462b147ca24e86baec72410a98a48a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c080feb1d6f04bd081d99ebc3d809eb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6405f67089c0463fa2f320f4f9359ae4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRH-XEteE8HU",
        "tags": []
      },
      "source": [
        "Check the shapes of all three parts of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaCCLJxAE8HU",
        "outputId": "e2af1432-d7bc-4777-af0d-767ba0ecc398",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of the datasets:\n",
            "Training: (12460, 3)\n",
            "Validation: (500, 3)\n",
            "Test: (1500, 3)\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 12460\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1500\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shapes of the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "print(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6lFrL5GE8HU"
      },
      "source": [
        "The output dataset is ready for fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3QtwYH3E8HU"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n",
        "\n",
        "Now utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTgkiDUmE8HU",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJovLriCUS4I",
        "outputId": "685a7063-e6bf-4f65-dd55-0aa4cf7b7e0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3200426141.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ],
      "source": [
        "import time, os, torch, transformers\n",
        "from packaging import version\n",
        "from transformers import (\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "tfv = version.parse(transformers.__version__)\n",
        "use_eval_key = \"eval_strategy\" if tfv >= version.parse(\"4.46.0\") else \"evaluation_strategy\"\n",
        "\n",
        "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "# Build kwargs with the right evaluation key for this transformers version\n",
        "args_kwargs = {\n",
        "    \"output_dir\": output_dir,\n",
        "    use_eval_key: \"epoch\",          # 'evaluation_strategy' or 'eval_strategy' depending on version\n",
        "    \"save_strategy\": \"epoch\",\n",
        "    \"logging_steps\": 25,\n",
        "    \"per_device_train_batch_size\": 8,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"gradient_accumulation_steps\": 2,\n",
        "    \"num_train_epochs\": 10,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \"bf16\": bool(use_bf16),\n",
        "    \"fp16\": not bool(use_bf16),\n",
        "    \"report_to\": [\"wandb\"],         # if W&B installed; harmless otherwise\n",
        "    \"predict_with_generate\": True,  # works with Seq2SeqTrainingArguments\n",
        "    \"generation_max_length\": 128,\n",
        "}\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(**args_kwargs)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=train_model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=train_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkF3b8v9E8HU",
        "tags": []
      },
      "source": [
        "Start training process...\n",
        "\n",
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5Zb3UgY2FuIHNhZmVseSBpZ25vcmUgdGhlIHdhcm5pbmcgbWVzc2FnZXMuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>\n",
        "\n",
        "The code trainer.train() utilizes the Weights & Biases (wandb) library to track and visualize the training process. To proceed, you'll need to sign up for a wandb account using your Gmail and then enter your unique API token to authenticate and enable logging of the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PY8xuEIoE8HU",
        "outputId": "b289f815-4a95-42e4-86ea-9fb0b2d649fa",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▂▁▂▁▁▁▁▁▂▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████████</td></tr><tr><td>train/loss</td><td>████▇▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.33097</td></tr><tr><td>eval/runtime</td><td>2.3148</td></tr><tr><td>eval/samples_per_second</td><td>216.002</td></tr><tr><td>eval/steps_per_second</td><td>27.216</td></tr><tr><td>train/epoch</td><td>2.31065</td></tr><tr><td>train/global_step</td><td>1800</td></tr><tr><td>train/grad_norm</td><td>1.77344</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>1.4793</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">full-ft-flan-t5-small-bf16-seq2seq</strong> at: <a href='https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5/runs/hyecc1xd' target=\"_blank\">https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5/runs/hyecc1xd</a><br> View project at: <a href='https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5' target=\"_blank\">https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251111_200826-hyecc1xd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251111_201501-m3zea50k</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5/runs/m3zea50k' target=\"_blank\">full-ft-flan-t5-small-bf16-seq2seq</a></strong> to <a href='https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5' target=\"_blank\">https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5/runs/m3zea50k' target=\"_blank\">https://wandb.ai/zubair-atha-columbia/dialogsum-flan-t5/runs/m3zea50k</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7790' max='7790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7790/7790 27:26, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.484900</td>\n",
              "      <td>1.304828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.475900</td>\n",
              "      <td>1.289082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.444800</td>\n",
              "      <td>1.283798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.454800</td>\n",
              "      <td>1.276436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.443900</td>\n",
              "      <td>1.275889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.440400</td>\n",
              "      <td>1.274485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.470900</td>\n",
              "      <td>1.272630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.437500</td>\n",
              "      <td>1.273003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.421300</td>\n",
              "      <td>1.272748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.396400</td>\n",
              "      <td>1.272845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/MountDrive/flan-diaglogue-summary-checkpoint/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/MountDrive/flan-diaglogue-summary-checkpoint/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/MountDrive/flan-diaglogue-summary-checkpoint/spiece.model',\n",
              " '/content/drive/MyDrive/MountDrive/flan-diaglogue-summary-checkpoint/added_tokens.json',\n",
              " '/content/drive/MyDrive/MountDrive/flan-diaglogue-summary-checkpoint/tokenizer.json')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Authenticate W&B (you'll be prompted for your API key the first time)\n",
        "wandb.login()\n",
        "\n",
        "# Optional: name the run and set a project\n",
        "wandb.init(project=\"dialogsum-flan-t5\", name=\"full-ft-flan-t5-small-bf16-seq2seq\", reinit=True)\n",
        "\n",
        "# Train\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Save checkpoint locally\n",
        "trainer.save_model(output_dir)          # saves model + adapter (if any)\n",
        "tokenizer.save_pretrained(output_dir)   # save tokenizer for completeness\n",
        "\n",
        "# (Optional) also save to Google Drive path for persistence\n",
        "drive_ckpt_dir = \"/content/drive/MyDrive/MountDrive/flan-diaglogue-summary-checkpoint\"\n",
        "os.makedirs(drive_ckpt_dir, exist_ok=True)\n",
        "trainer.save_model(drive_ckpt_dir)\n",
        "tokenizer.save_pretrained(drive_ckpt_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scp0s9yWE8HU",
        "tags": []
      },
      "source": [
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGXYtPR6E8HU",
        "tags": []
      },
      "source": [
        "Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5-DwWMO1YJl"
      },
      "outputs": [],
      "source": [
        "# prompt: I need to mount my google drive folder, MountDrive,  within which, I got a flan-diaglogue-summary-checkpoint folder, where the model checkpoint has\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDxqOv1EE8HX",
        "outputId": "ada59d08-9524-4098-f5e5-3f285384b94d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load tokenizer and models\n",
        "# Import T5Tokenizer from transformers\n",
        "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Define the model path using the config.json path\n",
        "model_path = \"/content/drive/MyDrive/MountDrive/flan-diaglogue-summary-checkpoint\"\n",
        "\n",
        "# Load tokenizer and models\n",
        "# Use the default T5 tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load the model in a way that is compatible with single-GPU environments\n",
        "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    # The following line addresses the multi-GPU loading issue\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Move model to GPU if available (optional, as device_map=\"auto\" should handle it)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "instruct_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13KUwIrPE8HX"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n",
        "\n",
        "As with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN7PKmm1dNdh",
        "outputId": "5155a994-a287-41aa-b4da-72da464b1371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "ORIGINAL MODEL:\n",
            "Is there anything else I can help you with?\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INSTRUCT MODEL:\n",
            "#Person2# considers adding a painting program to #Person2#'s software. #Person1# suggests adding a painting program to #Person2#'s software. #Person2# suggests adding a CD-ROM drive to #Person2#'s software.\n"
          ]
        }
      ],
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"Summarize the following conversation.\\n\\n{dialogue}\\n\\nSummary:\"\n",
        "# had issues with tensors so created a cleaner pipeline.\n",
        "# Put models in eval mode\n",
        "original_model.eval()\n",
        "instruct_model.eval()\n",
        "\n",
        "# Build inputs on each model's actual device\n",
        "orig_device = next(original_model.parameters()).device\n",
        "inst_device = next(instruct_model.parameters()).device\n",
        "\n",
        "orig_inputs = tokenizer(prompt, return_tensors=\"pt\").to(orig_device)\n",
        "inst_inputs = tokenizer(prompt, return_tensors=\"pt\").to(inst_device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    original_model_outputs = original_model.generate(\n",
        "        **orig_inputs,\n",
        "        max_new_tokens=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    instruct_model_outputs = instruct_model.generate(\n",
        "        **inst_inputs,\n",
        "        max_new_tokens=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-SOzk2wd3wf"
      },
      "source": [
        "* Relevance: improved from a generic, off-task utterance (“Is there anything else…”) to dialogue-specific content (mentions CD-ROM and speaker turns), showing the fine-tuned model is now attending to the actual conversation.\n",
        "* Coherence: shifts from a single vague sentence to a snippet that reflects turn-taking, but the output remains fragmentary and not yet a fluent, unified summary.\n",
        "* Coverage/abstraction: still misses the main instructional theme (software and hardware upgrades) captured by the human reference; it extracts a salient detail but doesn’t abstract or generalize to the overarching gist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzEGeg28E8HX"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
        "\n",
        "The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2b40efc2a6a644cdac53e7744a04edce",
            "932d5dff4c7f47a99f6c5ed776ec4e5c",
            "4ae84f28812c4d22bbc0a0a9345cb336",
            "f6126392a5cf40f58c1cced64eae7fd2",
            "013e51baae874d66ad84f803838512a0",
            "a90e02a82c9b40378b0f64a45dd9578c",
            "8ddd5b6fb00942f2b842bf7a777229be",
            "e92303a2d8dc49bd866d5241147b26b1",
            "dba49ee2588b442e863f8f412a07ce3b",
            "ef20a0a0faa14b6283fc9866a6791cc3",
            "7b07534622764dcd96f245b220773b09"
          ]
        },
        "id": "Dov5XGvVE8HX",
        "outputId": "81385197-f638-44a8-9263-9a8ac0c3b0fa",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b40efc2a6a644cdac53e7744a04edce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rouge = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G64-aLKyE8HX"
      },
      "source": [
        "Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "iSOhT4bme6mI",
        "outputId": "b27cfd29-86f7-40c7-c014-53b28d8f27f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"human_baseline_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched\",\n          \"In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\",\n          \"#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_model_summaries\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Talk to a friend.\",\n          \"Brian, thank you for inviting me to the party.\",\n          \"Ms. Dawson, Attached is a draft memo to all employees.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"instruct_model_summaries\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"#Person2# got stuck in traffic again because there was a terrible traffic jam near the Carrefour intersection. #Person1# thinks it would be better if #Person2# started taking public transport system to work. #Person2# thinks biking to work would be better for the environment.\",\n          \"Brian invites #Person1# to have a dance with #Person2#. #Person2# thinks #Person1#'s necklace goes with #Person1#'s dress.\",\n          \"Ms. Dawson asks Ms. Dawson to take a dictation for Ms. Dawson. Ms. Dawson tells Ms. Dawson that all office communications are restricted to email correspondence and official memos. Ms. Dawson tells Ms. Dawson that the use of instant messaging programs by employees during working hours is strictly prohibited. Ms. Dawson advises Ms. Dawson to change their communication methods.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-0a660237-7a66-4d30-9f62-c4d062c9f907\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>human_baseline_summaries</th>\n",
              "      <th>original_model_summaries</th>\n",
              "      <th>instruct_model_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
              "      <td>Ms. Dawson, Attached is a draft memo to all em...</td>\n",
              "      <td>Ms. Dawson asks Ms. Dawson to take a dictation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In order to prevent employees from wasting tim...</td>\n",
              "      <td>Ms. Dawson, Attached is a draft memo to all em...</td>\n",
              "      <td>Ms. Dawson asks Ms. Dawson to take a dictation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
              "      <td>Ms. Dawson, Attached is a draft memo to all em...</td>\n",
              "      <td>Ms. Dawson asks Ms. Dawson to take a dictation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#Person2# arrives late because of traffic jam....</td>\n",
              "      <td>Talk to a friend.</td>\n",
              "      <td>#Person2# got stuck in traffic again because t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
              "      <td>Talk to a friend.</td>\n",
              "      <td>#Person2# got stuck in traffic again because t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
              "      <td>Talk to a friend.</td>\n",
              "      <td>#Person2# got stuck in traffic again because t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
              "      <td>Kate, you're right. They are getting divorced.</td>\n",
              "      <td>Kate tells #Person1# that Masha and Hero are g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
              "      <td>Kate, you're right. They are getting divorced.</td>\n",
              "      <td>Kate tells #Person1# that Masha and Hero are g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
              "      <td>Kate, you're right. They are getting divorced.</td>\n",
              "      <td>Kate tells #Person1# that Masha and Hero are g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
              "      <td>Brian, thank you for inviting me to the party.</td>\n",
              "      <td>Brian invites #Person1# to have a dance with #...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a660237-7a66-4d30-9f62-c4d062c9f907')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0a660237-7a66-4d30-9f62-c4d062c9f907 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0a660237-7a66-4d30-9f62-c4d062c9f907');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c0e2e080-dec1-4168-8ef3-83b8e468ed21\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c0e2e080-dec1-4168-8ef3-83b8e468ed21')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c0e2e080-dec1-4168-8ef3-83b8e468ed21 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_1d569ad1-c48d-4700-ad7c-164caf3272fb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1d569ad1-c48d-4700-ad7c-164caf3272fb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                            human_baseline_summaries  \\\n",
              "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
              "1  In order to prevent employees from wasting tim...   \n",
              "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
              "3  #Person2# arrives late because of traffic jam....   \n",
              "4  #Person2# decides to follow #Person1#'s sugges...   \n",
              "5  #Person2# complains to #Person1# about the tra...   \n",
              "6  #Person1# tells Kate that Masha and Hero get d...   \n",
              "7  #Person1# tells Kate that Masha and Hero are g...   \n",
              "8  #Person1# and Kate talk about the divorce betw...   \n",
              "9  #Person1# and Brian are at the birthday party ...   \n",
              "\n",
              "                            original_model_summaries  \\\n",
              "0  Ms. Dawson, Attached is a draft memo to all em...   \n",
              "1  Ms. Dawson, Attached is a draft memo to all em...   \n",
              "2  Ms. Dawson, Attached is a draft memo to all em...   \n",
              "3                                  Talk to a friend.   \n",
              "4                                  Talk to a friend.   \n",
              "5                                  Talk to a friend.   \n",
              "6     Kate, you're right. They are getting divorced.   \n",
              "7     Kate, you're right. They are getting divorced.   \n",
              "8     Kate, you're right. They are getting divorced.   \n",
              "9     Brian, thank you for inviting me to the party.   \n",
              "\n",
              "                            instruct_model_summaries  \n",
              "0  Ms. Dawson asks Ms. Dawson to take a dictation...  \n",
              "1  Ms. Dawson asks Ms. Dawson to take a dictation...  \n",
              "2  Ms. Dawson asks Ms. Dawson to take a dictation...  \n",
              "3  #Person2# got stuck in traffic again because t...  \n",
              "4  #Person2# got stuck in traffic again because t...  \n",
              "5  #Person2# got stuck in traffic again because t...  \n",
              "6  Kate tells #Person1# that Masha and Hero are g...  \n",
              "7  Kate tells #Person1# that Masha and Hero are g...  \n",
              "8  Kate tells #Person1# that Masha and Hero are g...  \n",
              "9  Brian invites #Person1# to have a dance with #...  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "\n",
        "# Ensure eval mode\n",
        "original_model.eval()\n",
        "instruct_model.eval()\n",
        "\n",
        "for _, dialogue in enumerate(dialogues):\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Move input_ids to the same device as the model\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    # Build inputs on each model's own device to avoid device mismatch\n",
        "    orig_inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(original_model.parameters()).device)\n",
        "    inst_inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(instruct_model.parameters()).device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        original_model_outputs = original_model.generate(\n",
        "            **orig_inputs,\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        instruct_model_outputs = instruct_model.generate(\n",
        "            **inst_inputs,\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "    instruct_model_summaries.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBJU2xWrE8HX",
        "tags": []
      },
      "source": [
        "Evaluate the models computing ROUGE metrics. Notice the improvement in the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxmRLVUlE8HY",
        "outputId": "7e6e17be-20f4-446e-ba2e-656259d1fbec",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': np.float64(0.19539205828561151), 'rouge2': np.float64(0.058424390424390425), 'rougeL': np.float64(0.18498835301683875), 'rougeLsum': np.float64(0.1850460696787533)}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': np.float64(0.3540038759521339), 'rouge2': np.float64(0.11536574829817323), 'rougeL': np.float64(0.26206880377275144), 'rougeLsum': np.float64(0.2626508394503257)}\n"
          ]
        }
      ],
      "source": [
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD0XMcq0nAKa"
      },
      "source": [
        "The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Noclr0cflcWc",
        "outputId": "ea513427-ab19-47a4-9980-8f05306d6540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': np.float64(0.22165605484675166), 'rouge2': np.float64(0.07072656558348324), 'rougeL': np.float64(0.19244657776840873), 'rougeLsum': np.float64(0.19235273036456957)}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': np.float64(0.4043243894173554), 'rouge2': np.float64(0.17053687348320012), 'rougeL': np.float64(0.3266152074751759), 'rougeLsum': np.float64(0.3266155064476724)}\n"
          ]
        }
      ],
      "source": [
        "# --- 2.4: Compute ROUGE for Original vs Instruct from the CSV ---\n",
        "\n",
        "# These columns should exist in dialogue-summary-training-results.csv\n",
        "required_cols = [\"human_baseline_summaries\", \"original_model_summaries\", \"instruct_model_summaries\"]\n",
        "missing = [c for c in required_cols if c not in results.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns for 2.4: {missing}\")\n",
        "\n",
        "human_baseline_summaries = results[\"human_baseline_summaries\"].astype(str).tolist()\n",
        "original_model_summaries = results[\"original_model_summaries\"].astype(str).tolist()\n",
        "instruct_model_summaries = results[\"instruct_model_summaries\"].astype(str).tolist()\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "\n",
        "print(\"ORIGINAL MODEL:\")\n",
        "print(original_model_results)\n",
        "print(\"INSTRUCT MODEL:\")\n",
        "print(instruct_model_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2fxkSRDnEm1"
      },
      "source": [
        "The results show substantial improvement in all ROUGE metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQXkOFepljOm",
        "outputId": "6bea00e6-6cb4-4642-bd26-c63392f9739d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\n",
            "rouge1: 18.27%\n",
            "rouge2: 9.98%\n",
            "rougeL: 13.42%\n",
            "rougeLsum: 13.43%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n",
        "improvement = (\n",
        "    np.array(list(instruct_model_results.values()))\n",
        "    - np.array(list(original_model_results.values()))\n",
        ")\n",
        "for key, value in zip(instruct_model_results.keys(), improvement):\n",
        "    print(f\"{key}: {value*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-6mCSfTpmxl"
      },
      "source": [
        "* Full fine-tuning substantially outperforms zero-shot: rouge1 ↑ ~18.3 pp, rouge2 ↑ ~10.0 pp, rougeL/Lsum ↑ ~13.4 pp, showing stronger content coverage and structure alignment.\n",
        "* The large rouge2 gain indicates improved multi-token phrasing/coherence rather than just unigram overlap.\n",
        "* Absolute scores (rouge1 ≈ 0.40, rougeL ≈ 0.33) are solid for a small model, with room to grow via more data/epochs, consistent decoding constraints, and longer source lengths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRg8eo5kE8HY"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "Now, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon.\n",
        "\n",
        "PEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n",
        "\n",
        "That said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lquq7QZHE8HY"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning\n",
        "\n",
        "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6Z6bZc_E8HY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"v\"],  # common for T5 attention\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4pr_9n2E8HY",
        "tags": []
      },
      "source": [
        "Add LoRA adapter layers/parameters to the original LLM to be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBQLfDUvE8HY",
        "outputId": "1dd8e61b-7e71-44b7-9194-60e77b1293e6",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters:     77,305,216\n",
            "Trainable parameters: 344,064 (0.45%)\n",
            "{'total_params': 77305216, 'trainable_params': 344064, 'trainable_pct': 0.445072166928555}\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "peft_model = get_peft_model(copy.deepcopy(original_model), lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnQte7O6YNQW"
      },
      "source": [
        "To note: reduced from 100% to 0.45%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k1GYVfjE8HY",
        "tags": []
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 - Train PEFT Adapter\n",
        "\n",
        "Define training arguments and create `Trainer` instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhjqog89E8HY",
        "outputId": "40019222-cf57-40eb-fc52-08bb0feb6916",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1367329285.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  peft_trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=20\n",
        ")\n",
        "\n",
        "data_collator_peft = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=peft_model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8,\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator_peft,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaBfFhwn2QlM",
        "outputId": "76cd974a-3a2e-420c-ca36-b8124651a699"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1089340784.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  peft_trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "import torch, time\n",
        "\n",
        "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    # use a fixed, reasonably large batch to reduce steps/epoch\n",
        "    per_device_train_batch_size=16,    # try 16–32 on T5-small\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,     # effective batch 32\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    # A100 speedups\n",
        "    bf16=bool(use_bf16),               # critical for speed on A100\n",
        "    fp16=not bool(use_bf16),\n",
        "    dataloader_pin_memory=True,\n",
        "\n",
        "    # keep overhead low\n",
        "    logging_steps=100,\n",
        "    save_steps=0,                      # don’t save mid-epoch\n",
        "    # (leave eval during training off by not setting evaluation_strategy and/or not calling trainer.evaluate())\n",
        ")\n",
        "\n",
        "data_collator_peft = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=peft_model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8,              # tensor core friendly\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator_peft,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVSCIsJoE8HY"
      },
      "source": [
        "Now everything is ready to train the PEFT adapter and save the model.\n",
        "\n",
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OmkpJgGmE8HY",
        "outputId": "567df796-65bd-48e1-f394-1e0ba789dce0",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7800' max='7800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7800/7800 34:19, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.448300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.409000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.404200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.416800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.415200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.391700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.386300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.391600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.378600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.373900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.352700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.357400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.356100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.365200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.326000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.345400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.348900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.330400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.326100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.328700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.319800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.316600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.326500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.300100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.308600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.307200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.309000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.314800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.295100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.287300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.300900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.296800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.296800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.283200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>1.302300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.283600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>1.279600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.270900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>1.276100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>1.280900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>1.289000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>1.281100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.263400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>1.273900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>1.278100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>1.260500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>1.262600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.279700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>1.274500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>1.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>1.272300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>1.262800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>1.266500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>1.243700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>1.257100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>1.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>1.256400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>1.266900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>1.249900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>1.247000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>1.253500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>1.242300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>1.267400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>1.256100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>1.251800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>1.236200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>1.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.244300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>1.247200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>1.253000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>1.252600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>1.238700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>1.244300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>1.262000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>1.245700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>1.234900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
              " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
              " './peft-dialogue-summary-checkpoint-local/spiece.model',\n",
              " './peft-dialogue-summary-checkpoint-local/added_tokens.json',\n",
              " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_trainer.train()\n",
        "\n",
        "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
        "\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKxvcKUTE8HY",
        "tags": []
      },
      "source": [
        "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAs-5gVWE8HY",
        "tags": []
      },
      "source": [
        "That training was performed on a subset of data. To load a fully trained PEFT model, read a checkpoint of a PEFT model from Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD-Ur6kFE8HY",
        "tags": []
      },
      "source": [
        "Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV-gCy4zE8HY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"google/flan-t5-small\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "# Update the PEFT model path\n",
        "peft_model_path = '/content/peft-dialogue-summary-training-1762896987/checkpoint-7800'\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(\n",
        "    peft_model_base,\n",
        "    peft_model_path,\n",
        "    is_trainable=False\n",
        ")\n",
        "\n",
        "# Move the entire peft_model to the device\n",
        "peft_model = peft_model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-gZkgsPE8HZ",
        "tags": []
      },
      "source": [
        "The number of trainable parameters will be `0` due to `is_trainable=False` setting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qovlgGOTE8HZ",
        "outputId": "db9919e6-6216-4eca-90f7-9522e34f9415",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters:     77,305,216\n",
            "Trainable parameters: 0 (0.00%)\n",
            "{'total_params': 77305216, 'trainable_params': 0, 'trainable_pct': 0.0}\n"
          ]
        }
      ],
      "source": [
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FbLJ-4sbS81"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf1dObS0eO9E",
        "outputId": "fa97568a-23d1-4597-9c8c-dffa04a730ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install missing dependencies for ROUGE\n",
        "!pip -q install rouge_score evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY3OX_F1glCL",
        "outputId": "f23b147b-633f-41b3-fccb-5d0391b04beb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained PEFT model (10-sample) ROUGE:\n",
            "{'rouge1': np.float64(0.31680536488045763), 'rouge2': np.float64(0.086154358695533), 'rougeL': np.float64(0.24079210604520665), 'rougeLsum': np.float64(0.24079247697704376)}\n"
          ]
        }
      ],
      "source": [
        "# Compute 10-sample ROUGE for the pretrained PEFT model from Drive (self-contained)\n",
        "\n",
        "import os, json, torch\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- Load dataset ---\n",
        "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
        "\n",
        "# --- Ensure tokenizer + PEFT model are available; if not, load from Drive ---\n",
        "peft_dir = \"/content/drive/MyDrive/peft-dialogue-summary-checkpoint-from-s3\"\n",
        "\n",
        "def load_peft_from_drive(peft_path):\n",
        "    # Read adapter config to detect base model (fallback to flan-t5-base for width 768)\n",
        "    cfg_path = os.path.join(peft_path, \"adapter_config.json\")\n",
        "    with open(cfg_path, \"r\") as f:\n",
        "        adapter_cfg = json.load(f)\n",
        "    base_name = adapter_cfg.get(\"base_model_name_or_path\", \"google/flan-t5-base\")\n",
        "    tok = AutoTokenizer.from_pretrained(base_name)\n",
        "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "    base = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        base_name,\n",
        "        torch_dtype=torch.bfloat16 if use_bf16 else None,\n",
        "    )\n",
        "    peft = PeftModel.from_pretrained(base, peft_path, is_trainable=False)\n",
        "    return tok, peft\n",
        "\n",
        "try:\n",
        "    tokenizer\n",
        "    peft_loaded_model\n",
        "except NameError:\n",
        "    tokenizer, peft_loaded_model = load_peft_from_drive(peft_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "peft_loaded_model = peft_loaded_model.to(device).eval()\n",
        "\n",
        "# --- ROUGE on first 10 test samples ---\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "preds, refs = [], []\n",
        "\n",
        "for i in range(10):\n",
        "    dialogue = dataset[\"test\"][i][\"dialogue\"]\n",
        "    ref = dataset[\"test\"][i][\"summary\"]\n",
        "    prompt = f\"Summarize the following conversation.\\n\\n{dialogue}\\n\\nSummary:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(peft_loaded_model.parameters()).device)\n",
        "    with torch.no_grad():\n",
        "        out = peft_loaded_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    preds.append(tokenizer.decode(out[0], skip_special_tokens=True))\n",
        "    refs.append(ref)\n",
        "\n",
        "peft_rouge = rouge.compute(predictions=preds, references=refs)\n",
        "print(\"Pretrained PEFT model (10-sample) ROUGE:\")\n",
        "print(peft_rouge)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCenwoBLE8HZ"
      },
      "source": [
        "<a name='3.3'></a>\n",
        "### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n",
        "\n",
        "Make inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH-PcLMow0O8",
        "outputId": "b2bd6a5e-3db5-4d7d-8944-959c08284c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "ORIGINAL MODEL:\n",
            "Is there anything else I can help you with?\n",
            "---------------------------------------------------------------------------------------------------\n",
            "INSTRUCT MODEL:\n",
            "#Person2# considers adding a painting program to #Person2#'s software. #Person1# suggests adding a painting program to #Person2#'s software. #Person2# suggests adding a CD-ROM drive to #Person2#'s software.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "PEFT MODEL: \n",
            "#Person1# suggests adding a painting program to #Person2#'s software. #Person2# thinks it would be a bonus. #Person1# suggests adding a CD-ROM drive and adding a CD-ROM drive.\n"
          ]
        }
      ],
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"Summarize the following conversation.\\n\\n{dialogue}\\n\\nSummary:\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Move input_ids to the same device as the model (not strictly needed below since we rebuild per-model)\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "# Put models in eval\n",
        "original_model.eval()\n",
        "instruct_model.eval()\n",
        "peft_model.eval()\n",
        "\n",
        "# Build inputs on each model's device to avoid device mismatches\n",
        "orig_inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(original_model.parameters()).device)\n",
        "inst_inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(instruct_model.parameters()).device)\n",
        "peft_inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(peft_model.parameters()).device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    original_model_outputs = original_model.generate(\n",
        "        **orig_inputs,\n",
        "        max_new_tokens=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    instruct_model_outputs = instruct_model.generate(\n",
        "        **inst_inputs,\n",
        "        max_new_tokens=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    peft_model_outputs = peft_model.generate(\n",
        "        **peft_inputs,\n",
        "        max_new_tokens=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL: \\n{peft_model_text_output}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohh6RIjT_g_e"
      },
      "source": [
        "* Relevance and coverage: both trained models are on-topic and extract salient details (painting program, CD-ROM) whereas zero-shot is off-task; however, neither trained model captures the overarching theme (“teaches how to upgrade software and hardware”) as cleanly as the human.\n",
        "* Coherence and fluency: the fully fine-tuned model is more fluent than zero-shot but shows redundancy and role confusion (“#Person2# suggests…” twice); the PEFT model is comparably readable but repeats phrases (“adding a CD-ROM drive”) and lacks a single, unified summary sentence.\n",
        "* Faithfulness and abstraction: both trained models are mostly faithful to dialogue events but over-focus on specific add-ons, under-abstracting the instructional aspect; full FT edges PEFT slightly on variety, while PEFT is close with minor repetition artifacts—suggesting a need for decoding constraints (no_repeat_ngram_size, min/max tokens) and possibly more training or prompt refinement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FUvF-VGE8HZ"
      },
      "source": [
        "<a name='3.4'></a>\n",
        "### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
        "Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "MZ-7JSIQxjty",
        "outputId": "d8d103c9-0713-4f05-f157-cb7146ff2403"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"human_baseline_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched\",\n          \"In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\",\n          \"#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_model_summaries\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Talk to a friend.\",\n          \"Brian, thank you for inviting me to the party.\",\n          \"Thank you, sir.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"instruct_model_summaries\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"#Person2# got stuck in traffic again because there was a terrible traffic jam near the Carrefour intersection. #Person1# thinks it would be better if #Person2# started taking public transport system to work. #Person2# thinks biking to work would be better for the environment.\",\n          \"Brian invites #Person1# to have a dance with #Person2#. #Person2# thinks #Person1#'s necklace goes with #Person1#'s dress and #Person1#'s necklace makes #Person1# look good.\",\n          \"Ms. Dawson asks Ms. Dawson to take a dictation for Ms. Dawson. Ms. Dawson tells Ms. Dawson that all office communications are restricted to email correspondence and official memos. Ms. Dawson tells Ms. Dawson that the use of instant messaging programs by employees during working hours is strictly prohibited. Ms. Dawson advises Ms. Dawson to change their communication methods.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"peft_model_summaries\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"#Person2# got stuck in traffic again because there was a terrible traffic jam near the Carrefour intersection. #Person1# suggests taking the public transport system to work. #Person2# feels bad about how much #Person2#'s car is adding to the pollution problem in this city.\",\n          \"#Person1# invites Brian to celebrate his birthday and invites him to have a dance.\",\n          \"#Person1# asks Ms. Dawson to take a dictation for Ms. Dawson. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos. Ms. Dawson tells #Person1# the use of instant Message programs by employees during working hours is strictly prohibited.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-d1a67b1e-3290-46b0-aaf6-1ed13e248675\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>human_baseline_summaries</th>\n",
              "      <th>original_model_summaries</th>\n",
              "      <th>instruct_model_summaries</th>\n",
              "      <th>peft_model_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
              "      <td>Thank you, sir.</td>\n",
              "      <td>Ms. Dawson asks Ms. Dawson to take a dictation...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In order to prevent employees from wasting tim...</td>\n",
              "      <td>Thank you, sir.</td>\n",
              "      <td>Ms. Dawson asks Ms. Dawson to take a dictation...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
              "      <td>Thank you, sir.</td>\n",
              "      <td>Ms. Dawson asks Ms. Dawson to take a dictation...</td>\n",
              "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#Person2# arrives late because of traffic jam....</td>\n",
              "      <td>Talk to a friend.</td>\n",
              "      <td>#Person2# got stuck in traffic again because t...</td>\n",
              "      <td>#Person2# got stuck in traffic again because t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
              "      <td>Talk to a friend.</td>\n",
              "      <td>#Person2# got stuck in traffic again because t...</td>\n",
              "      <td>#Person2# got stuck in traffic again because t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
              "      <td>Talk to a friend.</td>\n",
              "      <td>#Person2# got stuck in traffic again because t...</td>\n",
              "      <td>#Person2# got stuck in traffic again because t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
              "      <td>Kate, you never believe what's happened.</td>\n",
              "      <td>Kate tells #Person1# that Masha and Hero are g...</td>\n",
              "      <td>Kate tells #Person1# Masha and Hero are gettin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
              "      <td>Kate, you never believe what's happened.</td>\n",
              "      <td>Kate tells #Person1# that Masha and Hero are g...</td>\n",
              "      <td>Kate tells #Person1# Masha and Hero are gettin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
              "      <td>Kate, you never believe what's happened.</td>\n",
              "      <td>Kate tells #Person1# that Masha and Hero are g...</td>\n",
              "      <td>Kate tells #Person1# Masha and Hero are gettin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
              "      <td>Brian, thank you for inviting me to the party.</td>\n",
              "      <td>Brian invites #Person1# to have a dance with #...</td>\n",
              "      <td>#Person1# invites Brian to celebrate his birth...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1a67b1e-3290-46b0-aaf6-1ed13e248675')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d1a67b1e-3290-46b0-aaf6-1ed13e248675 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d1a67b1e-3290-46b0-aaf6-1ed13e248675');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7ee541a2-4ea6-4793-96ad-5114091321f6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ee541a2-4ea6-4793-96ad-5114091321f6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7ee541a2-4ea6-4793-96ad-5114091321f6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8669d002-63b0-498f-b765-56c3ca533db5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8669d002-63b0-498f-b765-56c3ca533db5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                            human_baseline_summaries  \\\n",
              "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
              "1  In order to prevent employees from wasting tim...   \n",
              "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
              "3  #Person2# arrives late because of traffic jam....   \n",
              "4  #Person2# decides to follow #Person1#'s sugges...   \n",
              "5  #Person2# complains to #Person1# about the tra...   \n",
              "6  #Person1# tells Kate that Masha and Hero get d...   \n",
              "7  #Person1# tells Kate that Masha and Hero are g...   \n",
              "8  #Person1# and Kate talk about the divorce betw...   \n",
              "9  #Person1# and Brian are at the birthday party ...   \n",
              "\n",
              "                         original_model_summaries  \\\n",
              "0                                 Thank you, sir.   \n",
              "1                                 Thank you, sir.   \n",
              "2                                 Thank you, sir.   \n",
              "3                               Talk to a friend.   \n",
              "4                               Talk to a friend.   \n",
              "5                               Talk to a friend.   \n",
              "6        Kate, you never believe what's happened.   \n",
              "7        Kate, you never believe what's happened.   \n",
              "8        Kate, you never believe what's happened.   \n",
              "9  Brian, thank you for inviting me to the party.   \n",
              "\n",
              "                            instruct_model_summaries  \\\n",
              "0  Ms. Dawson asks Ms. Dawson to take a dictation...   \n",
              "1  Ms. Dawson asks Ms. Dawson to take a dictation...   \n",
              "2  Ms. Dawson asks Ms. Dawson to take a dictation...   \n",
              "3  #Person2# got stuck in traffic again because t...   \n",
              "4  #Person2# got stuck in traffic again because t...   \n",
              "5  #Person2# got stuck in traffic again because t...   \n",
              "6  Kate tells #Person1# that Masha and Hero are g...   \n",
              "7  Kate tells #Person1# that Masha and Hero are g...   \n",
              "8  Kate tells #Person1# that Masha and Hero are g...   \n",
              "9  Brian invites #Person1# to have a dance with #...   \n",
              "\n",
              "                                peft_model_summaries  \n",
              "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
              "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
              "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
              "3  #Person2# got stuck in traffic again because t...  \n",
              "4  #Person2# got stuck in traffic again because t...  \n",
              "5  #Person2# got stuck in traffic again because t...  \n",
              "6  Kate tells #Person1# Masha and Hero are gettin...  \n",
              "7  Kate tells #Person1# Masha and Hero are gettin...  \n",
              "8  Kate tells #Person1# Masha and Hero are gettin...  \n",
              "9  #Person1# invites Brian to celebrate his birth...  "
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "\n",
        "# Ensure eval mode\n",
        "original_model.eval()\n",
        "instruct_model.eval()\n",
        "peft_model.eval()\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    human_baseline_text_output = human_baseline_summaries[idx]\n",
        "\n",
        "    # Move input_ids to the same device as the model\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    # Build per-model inputs to avoid device mismatch\n",
        "    orig_inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(original_model.parameters()).device)\n",
        "    inst_inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(instruct_model.parameters()).device)\n",
        "    peft_inputs = tokenizer(prompt, return_tensors=\"pt\").to(next(peft_model.parameters()).device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        original_model_outputs = original_model.generate(\n",
        "            **orig_inputs,\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        instruct_model_outputs = instruct_model.generate(\n",
        "            **inst_inputs,\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        peft_model_outputs = peft_model.generate(\n",
        "            **peft_inputs,\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "    instruct_model_summaries.append(instruct_model_text_output)\n",
        "    peft_model_summaries.append(peft_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBF2CbalE8HZ",
        "outputId": "fa017d0a-e7bf-4123-ef75-3c2edd2556a6",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': np.float64(0.06935286935286936), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.06984348180000355), 'rougeLsum': np.float64(0.07148962148962149)}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': np.float64(0.35452568854973643), 'rouge2': np.float64(0.1144133659226495), 'rougeL': np.float64(0.2641151822349985), 'rougeLsum': np.float64(0.2639443107406383)}\n",
            "PEFT MODEL:\n",
            "{'rouge1': np.float64(0.3681963969164479), 'rouge2': np.float64(0.1201019958232719), 'rougeL': np.float64(0.28318007623291863), 'rougeLsum': np.float64(0.2844835147857303)}\n"
          ]
        }
      ],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTXl5L6KE8HZ"
      },
      "source": [
        "Notice, that PEFT model results are not too bad, while the training process was much easier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjuy-p3bmtWT"
      },
      "source": [
        "You already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7TBYmAWlrAh",
        "outputId": "6d1db67b-2749-44a5-8251-33ee374eb1e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': np.float64(0.22165605484675166), 'rouge2': np.float64(0.07072656558348324), 'rougeL': np.float64(0.19244657776840873), 'rougeLsum': np.float64(0.19235273036456957)}\n",
            "INSTRUCT MODEL:\n",
            "{'rouge1': np.float64(0.4043243894173554), 'rouge2': np.float64(0.17053687348320012), 'rougeL': np.float64(0.3266152074751759), 'rougeLsum': np.float64(0.3266155064476724)}\n",
            "PEFT MODEL:\n",
            "{'rouge1': np.float64(0.3911477954191801), 'rouge2': np.float64(0.15464524150189646), 'rougeL': np.float64(0.3136648257448643), 'rougeLsum': np.float64(0.3136251720757611)}\n"
          ]
        }
      ],
      "source": [
        "# Require the PEFT column as well for this section\n",
        "required_cols_34 = [\"human_baseline_summaries\", \"original_model_summaries\", \"instruct_model_summaries\", \"peft_model_summaries\"]\n",
        "missing_34 = [c for c in required_cols_34 if c not in results.columns]\n",
        "\n",
        "if missing_34:\n",
        "    raise KeyError(\n",
        "        \"Missing required columns for 3.4: \"\n",
        "        + \", \".join(missing_34)\n",
        "        + \"\\nMake sure your CSV includes 'peft_model_summaries'.\"\n",
        "    )\n",
        "\n",
        "human_baseline_summaries = results[\"human_baseline_summaries\"].astype(str).tolist()\n",
        "original_model_summaries = results[\"original_model_summaries\"].astype(str).tolist()\n",
        "instruct_model_summaries = results[\"instruct_model_summaries\"].astype(str).tolist()\n",
        "peft_model_summaries     = results[\"peft_model_summaries\"].astype(str).tolist()\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        ")\n",
        "\n",
        "print(\"ORIGINAL MODEL:\")\n",
        "print(original_model_results)\n",
        "print(\"INSTRUCT MODEL:\")\n",
        "print(instruct_model_results)\n",
        "print(\"PEFT MODEL:\")\n",
        "print(peft_model_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is2-2u70mxF5"
      },
      "source": [
        "The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n",
        "\n",
        "Calculate the improvement of PEFT over the original model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1AyzhSNmm5A",
        "outputId": "49372cc1-a671-4ebb-9bf9-ab777cca6339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
            "rouge1: 16.95%\n",
            "rouge2: 8.39%\n",
            "rougeL: 12.12%\n",
            "rougeLsum: 12.13%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nAbsolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
        "imp_peft_vs_orig = (\n",
        "    np.array(list(peft_model_results.values()))\n",
        "    - np.array(list(original_model_results.values()))\n",
        ")\n",
        "for key, value in zip(peft_model_results.keys(), imp_peft_vs_orig):\n",
        "    print(f\"{key}: {value*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icy0Cd3lmzwb"
      },
      "source": [
        "Now calculate the improvement of PEFT over a full fine-tuned model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDx99uiwm2ni",
        "outputId": "e9939956-9ae5-4059-d4d1-88ae1bd556ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\n",
            "rouge1: -1.32%\n",
            "rouge2: -1.59%\n",
            "rougeL: -1.30%\n",
            "rougeLsum: -1.30%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nAbsolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
        "imp_peft_vs_instr = (\n",
        "    np.array(list(peft_model_results.values()))\n",
        "    - np.array(list(instruct_model_results.values()))\n",
        ")\n",
        "for key, value in zip(peft_model_results.keys(), imp_peft_vs_instr):\n",
        "    print(f\"{key}: {value*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av11bSCnpf5c"
      },
      "source": [
        "* PEFT also beats zero-shot convincingly: rouge1 ↑ ~17.0 pp, rouge2 ↑ ~8.4 pp, rougeL/Lsum ↑ ~12.1 pp—adapters learn the summarization task effectively.\n",
        "* Versus full fine-tuning, PEFT is slightly behind (≈1.3–1.6 pp across metrics), a small gap that can vary with seeds/decoding.\n",
        "* Given PEFT’s tiny trainable footprint and small adapter checkpoints, its near-parity offers a strong efficiency–performance trade-off for rapid iteration and multi-tenant deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjgVoJW-qXGi"
      },
      "source": [
        "* Quality takeaways: both trained models improve n-gram overlap (R1/R2) and structure alignment (RL/Lsum) versus zero-shot; PEFT’s slight lead suggests LoRA captured task-specific patterns efficiently without overfitting.\n",
        "* Compute/efficiency trade-off: PEFT trained ~0.45% of weights (adapters only), yielding tiny checkpoints and lower memory needs; wall-clock per step remains similar (full forward pass still happens), but total compute and storage are far lower and adapters are stackable/reusable.\n",
        "* When to prefer each: choose PEFT for rapid iteration, multi-tenant adapters, limited VRAM, or deployment with shared base models; choose full FT if you control the whole model and need maximum headroom on very large domain shifts—accepting higher storage/maintenance costs.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "colab": {
      "collapsed_sections": [
        "4FbLJ-4sbS81"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "instance_type": "ml.m5.2xlarge",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "013e51baae874d66ad84f803838512a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02417dda2c2a463eb3886a0a3596836a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b895f533d3d4dfd87cf9ff7759a4c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e73fd41e0c854c7abaedf8c8c04528d5",
            "placeholder": "​",
            "style": "IPY_MODEL_f180ea1775e54f1e8bf809878413a3a9",
            "value": " 1500/1500 [00:00&lt;00:00, 4359.75 examples/s]"
          }
        },
        "19eb1b4f77a94691b88cef2a8de8f097": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5fc23f1534459aa8c9b3576a723689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29fb69504ff9405b8fd81e5ca6f57d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fda50113aaf449ffa5cc67aea81e06b3",
            "placeholder": "​",
            "style": "IPY_MODEL_1c5fc23f1534459aa8c9b3576a723689",
            "value": " 500/500 [00:00&lt;00:00, 4193.69 examples/s]"
          }
        },
        "2b40efc2a6a644cdac53e7744a04edce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_932d5dff4c7f47a99f6c5ed776ec4e5c",
              "IPY_MODEL_4ae84f28812c4d22bbc0a0a9345cb336",
              "IPY_MODEL_f6126392a5cf40f58c1cced64eae7fd2"
            ],
            "layout": "IPY_MODEL_013e51baae874d66ad84f803838512a0"
          }
        },
        "2ce72f50038e4d0d84c0c022e05e90eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fbdb2c5cb8149068e097ada3221e60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fcd598963784592aff8de79099c5383",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a35ead1a9bc4e99a9efd5a3d2aa8a00",
            "value": 500
          }
        },
        "2fcd598963784592aff8de79099c5383": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bce58e8e3944dbd9c275fb1bb9d2bab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d462b147ca24e86baec72410a98a48a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af3eb36c95844729aa02e1d45e744b07",
              "IPY_MODEL_b411b01ec5544a03a37480ff486e6649",
              "IPY_MODEL_40401be4ab774ca581cef4ecea6d09f5"
            ],
            "layout": "IPY_MODEL_a10769ea271c4d2f9ee76c50e3b417e4"
          }
        },
        "3ebbac34dad2492d91df8e9fa3790a9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40401be4ab774ca581cef4ecea6d09f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ebbac34dad2492d91df8e9fa3790a9f",
            "placeholder": "​",
            "style": "IPY_MODEL_aa406bb7d59b42eda7a9049830257503",
            "value": " 12460/12460 [00:02&lt;00:00, 4484.13 examples/s]"
          }
        },
        "415762945a0c48308adaa29b6ce7f74f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4474188b650e4b048e934fb6bc66454d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ae84f28812c4d22bbc0a0a9345cb336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e92303a2d8dc49bd866d5241147b26b1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dba49ee2588b442e863f8f412a07ce3b",
            "value": 1
          }
        },
        "4f733ba5662f44d4bc47ddde33f71a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "509e243000c14d6ca05af965c7e14d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcb64abd44144b848992ba67e75aff2a",
            "placeholder": "​",
            "style": "IPY_MODEL_9d8206dce5f64c7290e9a39a81554ee7",
            "value": "Map: 100%"
          }
        },
        "5e4ec3ab9c004ee4bff8004109e5ce0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ec502a62e8a45faba52ea99c9c10bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e4ec3ab9c004ee4bff8004109e5ce0a",
            "max": 1500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d015f1f922a4441905d769dac14fb4b",
            "value": 1500
          }
        },
        "6405f67089c0463fa2f320f4f9359ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa9d161841a54d568ec1757e8596cf08",
              "IPY_MODEL_5ec502a62e8a45faba52ea99c9c10bfa",
              "IPY_MODEL_0b895f533d3d4dfd87cf9ff7759a4c6e"
            ],
            "layout": "IPY_MODEL_19eb1b4f77a94691b88cef2a8de8f097"
          }
        },
        "64f9759fd57c4afa95b984edebf319f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70e6bf694a78425ebca526376d090744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9410398ecda2409584fe92a00e02c426",
            "placeholder": "​",
            "style": "IPY_MODEL_2ce72f50038e4d0d84c0c022e05e90eb",
            "value": "Filter: 100%"
          }
        },
        "76fdb2ec372a49908a14376d7ef64d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1a3a73fbb3541d2b633c7b5760d9f4c",
            "placeholder": "​",
            "style": "IPY_MODEL_02417dda2c2a463eb3886a0a3596836a",
            "value": " 500/500 [00:00&lt;00:00, 772.12 examples/s]"
          }
        },
        "7a35ead1a9bc4e99a9efd5a3d2aa8a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b07534622764dcd96f245b220773b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a84bc0015e74b3c8cd337897f28b572": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d015f1f922a4441905d769dac14fb4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ddd5b6fb00942f2b842bf7a777229be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "932d5dff4c7f47a99f6c5ed776ec4e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a90e02a82c9b40378b0f64a45dd9578c",
            "placeholder": "​",
            "style": "IPY_MODEL_8ddd5b6fb00942f2b842bf7a777229be",
            "value": "Downloading builder script: "
          }
        },
        "940e8a0b4232469a9ac4bd206bb7dec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbf269c44f7f422e9df9132b01bc99d3",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64f9759fd57c4afa95b984edebf319f7",
            "value": 500
          }
        },
        "9410398ecda2409584fe92a00e02c426": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d8206dce5f64c7290e9a39a81554ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a10769ea271c4d2f9ee76c50e3b417e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3cda832b276441ca531891e79a40877": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a90e02a82c9b40378b0f64a45dd9578c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa406bb7d59b42eda7a9049830257503": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa9d161841a54d568ec1757e8596cf08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b01d3b7a276344528de145e07e73a98f",
            "placeholder": "​",
            "style": "IPY_MODEL_4474188b650e4b048e934fb6bc66454d",
            "value": "Filter: 100%"
          }
        },
        "af3eb36c95844729aa02e1d45e744b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a84bc0015e74b3c8cd337897f28b572",
            "placeholder": "​",
            "style": "IPY_MODEL_c9fe8cf16a55403b94b9b06eff4ada28",
            "value": "Filter: 100%"
          }
        },
        "b01d3b7a276344528de145e07e73a98f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b411b01ec5544a03a37480ff486e6649": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_415762945a0c48308adaa29b6ce7f74f",
            "max": 12460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f733ba5662f44d4bc47ddde33f71a56",
            "value": 12460
          }
        },
        "c080feb1d6f04bd081d99ebc3d809eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70e6bf694a78425ebca526376d090744",
              "IPY_MODEL_940e8a0b4232469a9ac4bd206bb7dec0",
              "IPY_MODEL_29fb69504ff9405b8fd81e5ca6f57d70"
            ],
            "layout": "IPY_MODEL_3bce58e8e3944dbd9c275fb1bb9d2bab"
          }
        },
        "c9fe8cf16a55403b94b9b06eff4ada28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dba49ee2588b442e863f8f412a07ce3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcb64abd44144b848992ba67e75aff2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e73fd41e0c854c7abaedf8c8c04528d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e92303a2d8dc49bd866d5241147b26b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ef20a0a0faa14b6283fc9866a6791cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f180ea1775e54f1e8bf809878413a3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1a3a73fbb3541d2b633c7b5760d9f4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f21a7234c9ca43edacf1b03273397104": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_509e243000c14d6ca05af965c7e14d5c",
              "IPY_MODEL_2fbdb2c5cb8149068e097ada3221e60d",
              "IPY_MODEL_76fdb2ec372a49908a14376d7ef64d5f"
            ],
            "layout": "IPY_MODEL_a3cda832b276441ca531891e79a40877"
          }
        },
        "f6126392a5cf40f58c1cced64eae7fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef20a0a0faa14b6283fc9866a6791cc3",
            "placeholder": "​",
            "style": "IPY_MODEL_7b07534622764dcd96f245b220773b09",
            "value": " 6.27k/? [00:00&lt;00:00, 672kB/s]"
          }
        },
        "fbf269c44f7f422e9df9132b01bc99d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fda50113aaf449ffa5cc67aea81e06b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
